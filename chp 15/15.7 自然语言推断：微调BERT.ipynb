{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-12T06:42:16.080866284Z",
     "start_time": "2023-07-12T06:42:14.849437870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cookie/.conda/envs/torch/lib/python3.10/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T06:42:16.083763836Z",
     "start_time": "2023-07-12T06:42:16.082690384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 加载预先训练好的BERT参数\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, key_size=256, query_size=256, value_size=256, hid_in_features=256, mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
    "    return bert, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T06:56:32.949522002Z",
     "start_time": "2023-07-12T06:56:32.908887880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "bert = vocab = load_pretrained_model('bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4, num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T06:58:30.549784915Z",
     "start_time": "2023-07-12T06:58:29.204965326Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T07:03:47.481601773Z",
     "start_time": "2023-07-12T07:03:47.439242065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/cookie/.conda/envs/torch/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/cookie/.conda/envs/torch/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_562245/2600106956.py\", line 30, in _mp_worker\n    token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\nTypeError: tuple indices must be integers or slices, not list\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m batch_size, max_len, num_workers \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m128\u001B[39m, d2l\u001B[38;5;241m.\u001B[39mget_dataloader_workers()\n\u001B[1;32m      3\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mdownload_extract(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSNLI\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m train_set \u001B[38;5;241m=\u001B[39m \u001B[43mSNLIBERTDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43md2l\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_snli\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m test_set \u001B[38;5;241m=\u001B[39m SNLIBERTDataset(d2l\u001B[38;5;241m.\u001B[39mread_snli(data_dir, \u001B[38;5;28;01mFalse\u001B[39;00m), max_len, vocab)\n\u001B[1;32m      6\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(train_set, batch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39mnum_workers)\n",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m, in \u001B[0;36mSNLIBERTDataset.__init__\u001B[0;34m(self, dataset, max_len, vocab)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab \u001B[38;5;241m=\u001B[39m vocab\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len \u001B[38;5;241m=\u001B[39m max_len\n\u001B[1;32m     11\u001B[0m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_token_ids, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_segments,\n\u001B[0;32m---> 12\u001B[0m  \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_lens) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_premise_hypothesis_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_token_ids)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[7], line 17\u001B[0m, in \u001B[0;36mSNLIBERTDataset._preprocess\u001B[0;34m(self, all_premise_hypothesis_tokens)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_preprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, all_premise_hypothesis_tokens):\n\u001B[1;32m     16\u001B[0m     pool \u001B[38;5;241m=\u001B[39m multiprocessing\u001B[38;5;241m.\u001B[39mPool(\u001B[38;5;241m4\u001B[39m)  \u001B[38;5;66;03m# 使用4个进程\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mp_worker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_premise_hypothesis_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     all_token_ids \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     19\u001B[0m         token_ids \u001B[38;5;28;01mfor\u001B[39;00m token_ids, segments, valid_len \u001B[38;5;129;01min\u001B[39;00m out]\n\u001B[1;32m     20\u001B[0m     all_segments \u001B[38;5;241m=\u001B[39m [segments \u001B[38;5;28;01mfor\u001B[39;00m token_ids, segments, valid_len \u001B[38;5;129;01min\u001B[39;00m out]\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.10/multiprocessing/pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[0;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.10/multiprocessing/pool.py:774\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 774\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "\u001B[0;31mTypeError\u001B[0m: tuple indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,num_workers=num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T07:05:23.003282192Z",
     "start_time": "2023-07-12T07:05:10.303364217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T07:04:25.955811737Z",
     "start_time": "2023-07-12T07:04:25.946629224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m net \u001B[38;5;241m=\u001B[39m \u001B[43mBERTClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbert\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m, in \u001B[0;36mBERTClassifier.__init__\u001B[0;34m(self, bert)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, bert):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28msuper\u001B[39m(BERTClassifier, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m \u001B[43mbert\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden \u001B[38;5;241m=\u001B[39m bert\u001B[38;5;241m.\u001B[39mhidden\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "net = BERTClassifier(bert)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T07:04:27.775103532Z",
     "start_time": "2023-07-12T07:04:27.774220832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
